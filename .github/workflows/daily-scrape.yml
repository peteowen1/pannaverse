name: Daily FBref Scrape

on:
  schedule:
    # Run at 6 AM UTC daily (after most matches complete)
    - cron: '0 6 * * *'
  workflow_dispatch:
    # Allow manual trigger from GitHub Actions UI

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 180  # 3 hour timeout for long scrapes

    env:
      GITHUB_PAT: ${{ secrets.GITHUB_TOKEN }}

    steps:
    - name: Checkout pannaverse with submodules
      uses: actions/checkout@v4
      with:
        submodules: recursive
        fetch-depth: 0

    - name: Setup R
      uses: r-lib/actions/setup-r@v2
      with:
        r-version: 'release'

    - name: Setup R dependencies
      uses: r-lib/actions/setup-r-dependencies@v2
      with:
        working-directory: panna
        extra-packages: |
          any::piggyback
          any::httr
          any::rvest

    - name: Create release if it doesn't exist
      run: |
        Rscript -e '
          library(piggyback)
          tryCatch({
            pb_list(repo = "peteowen1/pannadata", tag = "latest")
            message("Release already exists")
          }, error = function(e) {
            message("Creating new release...")
            pb_new_release(repo = "peteowen1/pannadata", tag = "latest")
          })
        '
      shell: bash

    - name: Download existing data from GitHub Releases
      run: |
        Rscript -e '
          library(piggyback)

          # Download existing pannadata cache (zipped)
          tryCatch({
            pb_download(
              file = "pannadata.zip",
              repo = "peteowen1/pannadata",
              tag = "latest",
              dest = "."
            )
            message("Downloaded existing data from GitHub Releases")

            # Unzip preserving directory structure
            unzip("pannadata.zip", exdir = "pannadata")
            file.remove("pannadata.zip")
            message("Extracted data to pannadata/")
          }, error = function(e) {
            message("No existing data to download (first run?): ", e$message)
            # Create empty data directory for first run
            dir.create("pannadata/data", recursive = TRUE, showWarnings = FALSE)
          })
        '
      shell: bash

    - name: Run incremental scrape
      run: |
        cd panna
        Rscript data-raw/daily_scrape.R
      shell: bash

    - name: Upload data to GitHub Releases
      run: |
        Rscript -e '
          library(piggyback)

          # Zip the data directory to preserve structure
          message("Zipping data directory...")
          zip_file <- "pannadata.zip"

          # Create zip with directory structure preserved
          old_wd <- setwd("pannadata")
          zip(file.path("..", zip_file), files = "data", extras = "-r")
          setwd(old_wd)

          zip_size <- file.size(zip_file) / (1024 * 1024)
          message(sprintf("Created %s (%.1f MB)", zip_file, zip_size))

          # Upload zipped data
          tryCatch({
            pb_upload(
              file = zip_file,
              repo = "peteowen1/pannadata",
              tag = "latest",
              overwrite = TRUE
            )
            message("Uploaded data to GitHub Releases")
          }, error = function(e) {
            stop("Failed to upload: ", e$message)
          })

          # Cleanup
          file.remove(zip_file)
        '
      shell: bash

    - name: Summary
      run: |
        echo "Daily scrape completed successfully"
        echo "Check pannadata repo releases for updated data"

        # Show what was scraped
        find pannadata/data -name "*.rds" | wc -l | xargs -I {} echo "Total RDS files: {}"
      shell: bash
